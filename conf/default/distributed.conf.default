[distributed]
enabled = no
# Only save reports without analyze samples
master_storage_only = no
remove_task_on_worker = no
# Remove failed job on workers
failed_clean = no
# Distributed CAPE database, to store nodes and tasks info.
# For production use PostgresSQL. Sqlite is DEV ONLY
db = sqlite:///dist.db
# Tries before declare node as dead and deactivate it
dead_count = 5
# number of threads witch will retrieve files
dist_threads = 4
# Enable server side VMs tags filtering
enable_tags = no
# Fetch data over REST API or NFS, see docs how to setup NFS
# NFS is ultra fast comparing to restapi.
# Disable both if you don't need to fetch data from workers
restapi = no
nfs = no
# Do not copy from workers following folders and files:
ignore_patterns = binary, dump_sorted.pcap, memory.dmp, logs
main_server_name = master

[NFS]
# Path will be $CAPE_ROOT/$nfs_mount_folder. Ex: /opt/CAPEv2/workers
mount_folder = workers
fstab_socket = /tmp/cape-fstab

# Google Cloud Platform
[GCP]
enabled = no
# Comma separated list of zones
zones = ""
project_id = ""
# rest usage instead of GCP python client
# https://cloud.google.com/docs/authentication/rest
# gcloud auth print-access-token
token = ""
# Seconds between try to discoverd new instances
autodiscovery = 600
# Instances should start with following name pattern
instance_name = cape-server
