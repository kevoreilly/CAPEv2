# Copyright (C) 2010-2015 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import binascii
import contextlib
import copy
import hashlib
import logging
import mmap
import os
import struct
import subprocess
from pathlib import Path
from typing import Any, Dict

from lib.cuckoo.common.defines import (
    PAGE_EXECUTE,
    PAGE_EXECUTE_READ,
    PAGE_EXECUTE_READWRITE,
    PAGE_EXECUTE_WRITECOPY,
    PAGE_GUARD,
    PAGE_NOACCESS,
    PAGE_READONLY,
    PAGE_READWRITE,
    PAGE_WRITECOPY,
)
from lib.cuckoo.common.integrations.parse_pe import IMAGE_FILE_MACHINE_AMD64, IsPEImage

try:
    import magic

    HAVE_MAGIC = True
except ImportError:
    HAVE_MAGIC = False

try:
    import pydeep

    HAVE_PYDEEP = True
except ImportError:
    HAVE_PYDEEP = False

try:
    import pyclamd

    HAVE_CLAMAV = True
except ImportError:
    HAVE_CLAMAV = False

try:
    import re2 as re
except ImportError:
    import re

try:
    import pefile

    HAVE_PEFILE = True
except ImportError:
    HAVE_PEFILE = False

try:
    import tlsh

    HAVE_TLSH = True
except ImportError:
    print("Missed dependency: pip3 install python-tlsh")
    HAVE_TLSH = False

log = logging.getLogger(__name__)

yara_error = {
    "1": "ERROR_INSUFFICIENT_MEMORY",
    "2": "ERROR_COULD_NOT_ATTACH_TO_PROCESS",
    "3": "ERROR_COULD_NOT_OPEN_FILE",
    "4": "ERROR_COULD_NOT_MAP_FILE",
    "6": "ERROR_INVALID_FILE",
    "7": "ERROR_CORRUPT_FILE",
    "8": "ERROR_UNSUPPORTED_FILE_VERSION",
    "9": "ERROR_INVALID_REGULAR_EXPRESSION",
    "10": "ERROR_INVALID_HEX_STRING",
    "11": "ERROR_SYNTAX_ERROR",
    "12": "ERROR_LOOP_NESTING_LIMIT_EXCEEDED",
    "13": "ERROR_DUPLICATED_LOOP_IDENTIFIER",
    "14": "ERROR_DUPLICATED_IDENTIFIER",
    "15": "ERROR_DUPLICATED_TAG_IDENTIFIER",
    "16": "ERROR_DUPLICATED_META_IDENTIFIER",
    "17": "ERROR_DUPLICATED_STRING_IDENTIFIER",
    "18": "ERROR_UNREFERENCED_STRING",
    "19": "ERROR_UNDEFINED_STRING",
    "20": "ERROR_UNDEFINED_IDENTIFIER",
    "21": "ERROR_MISPLACED_ANONYMOUS_STRING",
    "22": "ERROR_INCLUDES_CIRCULAR_REFERENCE",
    "23": "ERROR_INCLUDE_DEPTH_EXCEEDED",
    "24": "ERROR_WRONG_TYPE",
    "25": "ERROR_EXEC_STACK_OVERFLOW",
    "26": "ERROR_SCAN_TIMEOUT",
    "27": "ERROR_TOO_MANY_SCAN_THREADS",
    "28": "ERROR_CALLBACK_ERROR",
    "29": "ERROR_INVALID_ARGUMENT",
    "30": "ERROR_TOO_MANY_MATCHES",
    "31": "ERROR_INTERNAL_FATAL_ERROR",
    "32": "ERROR_NESTED_FOR_OF_LOOP",
    "33": "ERROR_INVALID_FIELD_NAME",
    "34": "ERROR_UNKNOWN_MODULE",
    "35": "ERROR_NOT_A_STRUCTURE",
    "36": "ERROR_NOT_INDEXABLE",
    "37": "ERROR_NOT_A_FUNCTION",
    "38": "ERROR_INVALID_FORMAT",
    "39": "ERROR_TOO_MANY_ARGUMENTS",
    "40": "ERROR_WRONG_ARGUMENTS",
    "41": "ERROR_WRONG_RETURN_TYPE",
    "42": "ERROR_DUPLICATED_STRUCTURE_MEMBER",
    "43": "ERROR_EMPTY_STRING",
    "44": "ERROR_DIVISION_BY_ZERO",
    "45": "ERROR_REGULAR_EXPRESSION_TOO_LARGE",
    "46": "ERROR_TOO_MANY_RE_FIBERS",
    "47": "ERROR_COULD_NOT_READ_PROCESS_MEMORY",
    "48": "ERROR_INVALID_EXTERNAL_VARIABLE_TYPE",
    "49": "ERROR_REGULAR_EXPRESSION_TOO_COMPLEX",
}

type_list = [
    "RAR self-extracting archive",
    "Nullsoft Installer self-extracting archive",
    "7-zip Installer data",
    "Inno Setup",
    "MSI Installer",
    "Microsoft Cabinet",  # ToDo add die support here
]


class Dictionary(dict):
    """Cuckoo custom dict."""

    def __getattr__(self, key):
        return self.get(key)

    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


class PCAP:
    """PCAP base object."""

    def __init__(self, file_path):
        self.file_path = file_path


class URL:
    """URL base object."""

    def __init__(self, url):
        """@param url: URL"""
        self.url = url


class File:
    """Basic file object class with all useful utilities."""

    # The yara rules should not change during one Cuckoo run and as such we're
    # caching 'em. This dictionary is filled during init_yara().
    yara_rules = {}
    yara_initialized = False
    # static fields which indicate whether the user has been
    # notified about missing dependencies already
    notified_yara = False
    notified_pydeep = False

    def __init__(self, file_path, guest_paths=None, file_name=None):
        """@param file_path: file path."""
        self.file_name = file_name
        self.file_path = file_path
        self.file_path_ansii = file_path if isinstance(file_path, str) else file_path.decode()
        self.guest_paths = guest_paths
        self.path_object = Path(self.file_path_ansii)

        # these will be populated when first accessed
        self._file_data = None
        self._crc32 = None
        self._md5 = None
        self._sha1 = None
        self._sha256 = None
        self._sha512 = None
        self._pefile = False
        self.file_type = None
        self.pe = None

    def get_name(self):
        """Get file name.
        @return: file name.
        """
        return self.file_name or Path(self.file_path).name

    def valid(self):
        return self.path_object.exists() and self.path_object.is_file() and self.path_object.stat().st_size

    def get_data(self):
        """Read file contents.
        @return: data.
        """
        return self.file_data

    def get_chunks(self, size=16):
        """Read file contents in chunks (generator)."""
        chunk_size = size * 1024
        with open(self.file_path, "rb") as fd:
            while True:
                chunk = fd.read(chunk_size)
                if not chunk:
                    break
                yield chunk

    def calc_hashes(self):
        """Calculate all possible hashes for this file."""
        crc = 0
        md5 = hashlib.md5()
        sha1 = hashlib.sha1()
        sha256 = hashlib.sha256()
        sha512 = hashlib.sha512()
        sha3_384 = hashlib.sha3_384()

        if HAVE_TLSH:
            tlsh_hash = tlsh.Tlsh()

        for chunk in self.get_chunks():
            crc = binascii.crc32(chunk, crc)
            md5.update(chunk)
            sha1.update(chunk)
            sha256.update(chunk)
            sha512.update(chunk)
            sha3_384.update(chunk)
            if HAVE_TLSH:
                tlsh_hash.update(chunk)

        self._crc32 = "".join(f"{(crc >> i) & 0xFF:02X}" for i in (24, 16, 8, 0))
        self._md5 = md5.hexdigest()
        self._sha1 = sha1.hexdigest()
        self._sha256 = sha256.hexdigest()
        self._sha512 = sha512.hexdigest()
        self._sha3_384 = sha3_384.hexdigest()
        if HAVE_TLSH:
            with contextlib.suppress(ValueError):
                tlsh_hash.final()
                self._tlsh_hash = tlsh_hash.hexdigest()

    @property
    def file_data(self):
        if not self._file_data:
            if self.path_object.exists():
                self._file_data = self.path_object.read_bytes()
        return self._file_data

    def get_size(self):
        """Get file size.
        @return: file size.
        """
        return self.path_object.stat().st_size if self.path_object.exists() else 0

    def get_crc32(self):
        """Get CRC32.
        @return: CRC32.
        """
        if not self._crc32:
            self.calc_hashes()
        return self._crc32

    def get_md5(self):
        """Get MD5.
        @return: MD5.
        """
        if not self._md5:
            self.calc_hashes()
        return self._md5

    def get_sha1(self):
        """Get SHA1.
        @return: SHA1.
        """
        if not self._sha1:
            self.calc_hashes()
        return self._sha1

    def get_sha256(self):
        """Get SHA256.
        @return: SHA256.
        """
        if not self._sha256:
            self.calc_hashes()
        return self._sha256

    def get_sha512(self):
        """
        Get SHA512.
        @return: SHA512.
        """
        if not self._sha512:
            self.calc_hashes()
        return self._sha512

    def get_sha3_384(self):
        """
        Get SHA3_384.
        @return: SHA3_384.
        """
        if not self._sha3_384:
            self.calc_hashes()
        return self._sha3_384

    def get_ssdeep(self):
        """Get SSDEEP.
        @return: SSDEEP.
        """
        if not HAVE_PYDEEP:
            if not File.notified_pydeep:
                File.notified_pydeep = True
                log.warning("Unable to import pydeep (install with `pip3 install pydeep`)")
            return None

        try:
            return pydeep.hash_file(self.file_path).decode()
        except Exception:
            return None

    def get_content_type(self):
        """Get MIME content file type (example: image/jpeg).
        @return: file content type.
        """
        file_type = None
        if self.path_object.exists():
            if HAVE_MAGIC:
                if hasattr(magic, "from_file"):
                    try:
                        file_type = magic.from_file(self.file_path_ansii)
                    except Exception as e:
                        log.error(e, exc_info=True)
                if not file_type and hasattr(magic, "open"):
                    try:
                        ms = magic.open(magic.MAGIC_MIME | magic.MAGIC_SYMLINK)
                        ms.load()
                        file_type = ms.file(self.file_path)
                        ms.close()
                    except Exception as e:
                        log.error(e, exc_info=True)

            if file_type is None:
                try:
                    p = subprocess.Popen(
                        ["file", "-b", "-L", "--mime-type", self.file_path], universal_newlines=True, stdout=subprocess.PIPE
                    )
                    file_type = p.stdout.read().strip()
                except Exception as e:
                    log.error(e, exc_info=True)

        return file_type

    def get_type(self):
        """Get MIME file type.
        @return: file type.
        """
        if self.file_type:
            return self.file_type
        if self.file_path:
            try:
                if IsPEImage(self.file_data):
                    self._pefile = True
                    if HAVE_PEFILE:
                        try:
                            self.pe = pefile.PE(data=self.file_data, fast_load=True)
                        except pefile.PEFormatError:
                            self.file_type = "PE image for MS Windows"
                            log.error("Unable to instantiate pefile on image")
                        if self.pe:
                            is_dll = self.pe.is_dll()
                            is_x64 = self.pe.FILE_HEADER.Machine == IMAGE_FILE_MACHINE_AMD64
                            gui_type = "console" if self.pe.OPTIONAL_HEADER.Subsystem == 3 else "GUI"
                            # Emulate magic for now
                            if is_dll and is_x64:
                                self.file_type = "PE32+ executable (DLL) (GUI) x86-64, for MS Windows"
                            elif is_dll:
                                self.file_type = "PE32 executable (DLL) (GUI) Intel 80386, for MS Windows"
                            elif is_x64:
                                self.file_type = f"PE32+ executable ({gui_type}) x86-64, for MS Windows"
                            else:
                                self.file_type = f"PE32 executable ({gui_type}) Intel 80386, for MS Windows"
                    elif not File.notified_pefile:
                        File.notified_pefile = True
                        log.warning("Unable to import pefile (install with `pip3 install pefile`)")
            except Exception as e:
                log.error(e, exc_info=True)
        if not self.file_type:
            self.file_type = self.get_content_type()

        return self.file_type

    def _yara_encode_string(self, yara_string):
        # Beware, spaghetti code ahead.
        try:
            new = yara_string.decode()
        except UnicodeDecodeError:
            # yara_string = binascii.hexlify(yara_string.lstrip("uU")).upper()
            yara_string = binascii.hexlify(yara_string).upper()
            yara_string = b" ".join(yara_string[i : i + 2] for i in range(0, len(yara_string), 2))
            new = f"{{ {yara_string.decode()} }}"

        return new

    def get_yara(self, category="binaries", externals=None):
        """Get Yara signatures matches.
        @return: matched Yara signatures.
        """
        results = []
        if not os.path.getsize(self.file_path):
            return results

        try:
            results, rule = [], File.yara_rules[category]
            for match in rule.match(self.file_path_ansii, externals=externals):
                strings = {self._yara_encode_string(s[2]) for s in match.strings}
                addresses = {s[1].strip("$"): s[0] for s in match.strings}
                results.append(
                    {
                        "name": match.rule,
                        "meta": match.meta,
                        "strings": list(strings),
                        "addresses": addresses,
                    }
                )
        except Exception as e:
            errcode = str(e).rsplit(maxsplit=1)[-1]
            if errcode in yara_error:
                log.exception("Unable to match Yara signatures for %s: %s", self.file_path, yara_error[errcode])

            else:
                log.exception("Unable to match Yara signatures for %s: unknown code %s", self.file_path, errcode)

        return results

    cape_name_regex = re.compile(r" (?:payload|config|loader)$", re.I)

    @classmethod
    def yara_hit_provides_detection(cls, hit: Dict[str, Any]) -> bool:
        cape_type = hit["meta"].get("cape_type", "")
        return bool(cls.cape_name_regex.search(cape_type))

    @classmethod
    def get_cape_name_from_yara_hit(cls, hit: Dict[str, Any]) -> str:
        """Use the cape_type as defined in the metadata for the yara hit
        (e.g. "SocGholish Payload") and return the part before
        "Loader", "Payload", or "Config".
        """
        return cls.get_cape_name_from_cape_type(hit["meta"].get("cape_type", ""))

    @classmethod
    def get_cape_name_from_cape_type(cls, cape_type: str) -> str:
        """Return the part of the cape_type (e.g. "SocGholish Payload") preceding
        " Payload", " Config", or " Loader".
        """
        return cls.cape_name_regex.sub("", cape_type)

    def get_clamav(self):
        """Get ClamAV signatures matches.
        Requires pyclamd module. Additionally if running with apparmor, an exception must be made.
        apt-get install clamav clamav-daemon clamav-freshclam clamav-unofficial-sigs -y
        pip3 install -U pyclamd
        systemctl enable clamav-daemon
        systemctl start clamav-daemon
        usermod -a -G cuckoo clamav
        echo "/opt/CAPEv2/storage/** r," | sudo tee -a /etc/apparmor.d/local/usr.sbin.clamd
        @return: matched ClamAV signatures.
        """
        matches = []

        if HAVE_CLAMAV and os.path.getsize(self.file_path) > 0:
            try:
                cd = pyclamd.ClamdUnixSocket()
                results = cd.allmatchscan(self.file_path)
                if results:
                    for entry in results[self.file_path]:
                        if entry[0] == "FOUND" and entry[1] not in matches:
                            matches.append(entry[1])
            except ConnectionError:
                log.warning("failed to connect to clamd socket")
            except Exception as e:
                log.warning("failed to scan file with clamav %s", e)
            finally:
                return matches
        return matches

    def get_tlsh(self):
        """
        Get TLSH.
        @return: TLSH.
        """
        if not hasattr(self, "_tlsh_hash"):
            return None
        if not self._tlsh_hash:
            self.calc_hashes()
        return self._tlsh_hash

    def get_rh_hash(self):
        if not self.pe:
            return None

        # source https://github.com/RichHeaderResearch/RichPE/blob/master/richpe.py#L34
        rich_header = self.pe.parse_rich_header()
        if rich_header is None:
            return None

        # Get list of @Comp.IDs and counts from Rich header
        # Elements in rich_fields at even indices are @Comp.IDs
        # Elements in rich_fields at odd indices are counts
        rich_fields = rich_header.get("values")
        if len(rich_fields) % 2 != 0:
            return None

        # The RichPE hash of a file is computed by computing the md5 of specific
        # metadata within  the Rich header and the PE header
        md5 = hashlib.md5()

        # Update hash using @Comp.IDs and masked counts from Rich header
        while len(rich_fields):
            compid = rich_fields.pop(0)
            count = rich_fields.pop(0)
            mask = 2 ** (count.bit_length() // 2 + 1) - 1
            count |= mask
            md5.update(struct.pack("<L", compid))
            md5.update(struct.pack("<L", count))

        # Update hash using metadata from the PE header
        md5.update(struct.pack("<L", self.pe.FILE_HEADER.Machine))
        md5.update(struct.pack("<L", self.pe.FILE_HEADER.Characteristics))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.Subsystem))
        md5.update(struct.pack("<B", self.pe.OPTIONAL_HEADER.MajorLinkerVersion))
        md5.update(struct.pack("<B", self.pe.OPTIONAL_HEADER.MinorLinkerVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MajorOperatingSystemVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MinorOperatingSystemVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MajorImageVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MinorImageVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MajorSubsystemVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MinorSubsystemVersion))

        # Close PE file and return RichPE hash digest
        return md5.hexdigest()

    def is_sfx(self):
        filetype = self.get_content_type()
        return any([ftype in filetype for ftype in type_list])

    def get_all_hashes(self):
        return {
            "crc32": self.get_crc32(),
            "md5": self.get_md5(),
            "sha1": self.get_sha1(),
            "sha256": self.get_sha256(),
            "sha512": self.get_sha512(),
            "rh_hash": self.get_rh_hash(),
            "ssdeep": self.get_ssdeep(),
            "tlsh": self.get_tlsh(),
            "sha3_384": self.get_sha3_384(),
        }

    def get_all(self):
        """Get all information available.
        @return: information dict.
        """

        infos = {
            "name": self.get_name(),
            "path": self.file_path,
            "guest_paths": self.guest_paths,
            "size": self.get_size(),
            "crc32": self.get_crc32(),
            "md5": self.get_md5(),
            "sha1": self.get_sha1(),
            "sha256": self.get_sha256(),
            "sha512": self.get_sha512(),
            "rh_hash": self.get_rh_hash(),
            "ssdeep": self.get_ssdeep(),
            "type": self.get_type(),
            "yara": self.get_yara(),
            "cape_yara": self.get_yara(category="CAPE"),
            "clamav": self.get_clamav(),
            "tlsh": self.get_tlsh(),
            "sha3_384": self.get_sha3_384(),
        }

        return infos, self.pe


class Static(File):
    pass


class ProcDump:
    def __init__(self, dump_file, pretty=False):
        self._dumpfile = open(dump_file, "rb")
        self.dumpfile = mmap.mmap(self._dumpfile.fileno(), 0, access=mmap.ACCESS_READ)
        self.address_space = self.parse_dump()
        self.protmap = {
            PAGE_NOACCESS: "NOACCESS",
            PAGE_READONLY: "R",
            PAGE_READWRITE: "RW",
            PAGE_WRITECOPY: "RWC",
            PAGE_EXECUTE: "X",
            PAGE_EXECUTE_READ: "RX",
            PAGE_EXECUTE_READWRITE: "RWX",
            PAGE_EXECUTE_WRITECOPY: "RWXC",
        }

    def __del__(self):
        self.close()

    def close(self):
        if self.dumpfile:
            self.dumpfile.close()
            self.dumpfile = None
        if self._dumpfile:
            self._dumpfile.close()
            self._dumpfile = None

    def _prot_to_str(self, prot):
        if prot & PAGE_GUARD:
            return "G"
        prot &= 0xFF
        return self.protmap[prot]

    def pretty_print(self):
        new_addr_space = copy.deepcopy(self.address_space)
        for map in new_addr_space:
            map["start"] = f"0x{map['start']:08x}"
            map["end"] = f"0x{map['end']:08x}"
            map["size"] = f"0x{map['size']:08x}"
            if map["prot"] is None:
                map["prot"] = "Mixed"
            else:
                map["prot"] = self._prot_to_str(map["prot"])
            for chunk in map["chunks"]:
                chunk["start"] = f"0x{chunk['start']:08x}"
                chunk["end"] = f"0x{chunk['end']:08x}"
                chunk["size"] = f"0x{chunk['size']:08x}"
                chunk["prot"] = self._prot_to_str(chunk["prot"])
        return new_addr_space

    def _coalesce_chunks(self, chunklist):
        low = chunklist[0]["start"]
        high = chunklist[-1]["end"]
        prot = chunklist[0]["prot"]
        PE = chunklist[0]["PE"]
        for chunk in chunklist:
            if chunk["prot"] != prot:
                prot = None
        return {"start": low, "end": high, "size": high - low, "prot": prot, "PE": PE, "chunks": chunklist}

    def parse_dump(self):
        f = self.dumpfile
        address_space = []
        curchunk = []
        lastend = 0
        while True:
            data = f.read(24)
            if data == b"":
                break
            addr, size, mem_state, mem_type, mem_prot = struct.unpack("QIIII", data)
            offset = f.tell()
            if addr != lastend and len(curchunk):
                address_space.append(self._coalesce_chunks(curchunk))
                curchunk = []
            lastend = addr + size
            alloc = {
                "start": addr,
                "end": addr + size,
                "size": size,
                "prot": mem_prot,
                "state": mem_state,
                "type": mem_type,
                "offset": offset,
                "PE": False,
            }

            try:
                if f.read(2) == b"MZ":
                    alloc["PE"] = True
                f.seek(size - 2, 1)
            except Exception:
                break
            curchunk.append(alloc)
        if len(curchunk):
            address_space.append(self._coalesce_chunks(curchunk))

        f.seek(0)

        return address_space

    def get_data(self, addr, size):
        for map in self.address_space:
            if addr < map["start"] or addr >= map["end"]:
                continue
            for chunk in map["chunks"]:
                if addr < chunk["start"] or addr >= chunk["end"]:
                    continue
                maxsize = chunk["start"] + chunk["size"] - addr
                if size > maxsize:
                    size = maxsize
                self.dumpfile.seek(chunk["offset"] + addr - chunk["start"])
                return self.dumpfile.read(size)

    def search(self, regex, flags=0, all=False):
        if all:
            result = {"detail": []}
            matches = []
            for map in self.address_space:
                for chunk in map["chunks"]:
                    self.dumpfile.seek(chunk["offset"])
                    match = re.finditer(regex, self.dumpfile.read(chunk["end"] - chunk["start"]), flags)
                    thismatch = []
                    with contextlib.suppress(StopIteration):
                        while True:
                            m = next(match)
                            thismatch.append(m)
                            matches.append(m.group(0))
                    if thismatch:
                        result["detail"].append({"match": thismatch, "chunk": chunk})
            result["matches"] = matches
            return result
        else:
            for map in self.address_space:
                for chunk in map["chunks"]:
                    self.dumpfile.seek(chunk["offset"])
                    match = re.search(regex, self.dumpfile.read(chunk["end"] - chunk["start"]), flags)
                    if match:
                        return {"match": match, "chunk": chunk}
