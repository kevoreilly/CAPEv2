# Copyright (C) 2010-2015 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import binascii
import contextlib
import copy
import hashlib
import logging
import mmap
import os
import struct
import subprocess
from pathlib import Path
from typing import Any, Dict

from lib.cuckoo.common.constants import CUCKOO_ROOT
from lib.cuckoo.common.defines import (
    PAGE_EXECUTE,
    PAGE_EXECUTE_READ,
    PAGE_EXECUTE_READWRITE,
    PAGE_EXECUTE_WRITECOPY,
    PAGE_GUARD,
    PAGE_NOACCESS,
    PAGE_READONLY,
    PAGE_READWRITE,
    PAGE_WRITECOPY,
)
from lib.cuckoo.common.integrations.clamav import get_clamav
from lib.cuckoo.common.integrations.parse_pe import IMAGE_FILE_MACHINE_AMD64, IsPEImage
from lib.cuckoo.common.path_utils import path_exists

try:
    import magic

    HAVE_MAGIC = True
except ImportError:
    HAVE_MAGIC = False

try:
    import pydeep

    HAVE_PYDEEP = True
except ImportError:
    HAVE_PYDEEP = False

try:
    import re2 as re
except ImportError:
    import re

try:
    import pefile

    HAVE_PEFILE = True
except ImportError:
    HAVE_PEFILE = False

try:
    import tlsh

    HAVE_TLSH = True
except ImportError:
    print("Missed dependency: poetry run pip install -r extra/optional_dependencies.txt")
    HAVE_TLSH = False

try:
    import yara

    HAVE_YARA = True
    if not int(yara.__version__[0]) >= 4:
        raise ImportError("Missed library. Run: poetry install")
except ImportError:
    print("Missed library. Run: poetry install")
    HAVE_YARA = False


log = logging.getLogger(__name__)

yara_error = {
    "1": "ERROR_INSUFFICIENT_MEMORY",
    "2": "ERROR_COULD_NOT_ATTACH_TO_PROCESS",
    "3": "ERROR_COULD_NOT_OPEN_FILE",
    "4": "ERROR_COULD_NOT_MAP_FILE",
    "6": "ERROR_INVALID_FILE",
    "7": "ERROR_CORRUPT_FILE",
    "8": "ERROR_UNSUPPORTED_FILE_VERSION",
    "9": "ERROR_INVALID_REGULAR_EXPRESSION",
    "10": "ERROR_INVALID_HEX_STRING",
    "11": "ERROR_SYNTAX_ERROR",
    "12": "ERROR_LOOP_NESTING_LIMIT_EXCEEDED",
    "13": "ERROR_DUPLICATED_LOOP_IDENTIFIER",
    "14": "ERROR_DUPLICATED_IDENTIFIER",
    "15": "ERROR_DUPLICATED_TAG_IDENTIFIER",
    "16": "ERROR_DUPLICATED_META_IDENTIFIER",
    "17": "ERROR_DUPLICATED_STRING_IDENTIFIER",
    "18": "ERROR_UNREFERENCED_STRING",
    "19": "ERROR_UNDEFINED_STRING",
    "20": "ERROR_UNDEFINED_IDENTIFIER",
    "21": "ERROR_MISPLACED_ANONYMOUS_STRING",
    "22": "ERROR_INCLUDES_CIRCULAR_REFERENCE",
    "23": "ERROR_INCLUDE_DEPTH_EXCEEDED",
    "24": "ERROR_WRONG_TYPE",
    "25": "ERROR_EXEC_STACK_OVERFLOW",
    "26": "ERROR_SCAN_TIMEOUT",
    "27": "ERROR_TOO_MANY_SCAN_THREADS",
    "28": "ERROR_CALLBACK_ERROR",
    "29": "ERROR_INVALID_ARGUMENT",
    "30": "ERROR_TOO_MANY_MATCHES",
    "31": "ERROR_INTERNAL_FATAL_ERROR",
    "32": "ERROR_NESTED_FOR_OF_LOOP",
    "33": "ERROR_INVALID_FIELD_NAME",
    "34": "ERROR_UNKNOWN_MODULE",
    "35": "ERROR_NOT_A_STRUCTURE",
    "36": "ERROR_NOT_INDEXABLE",
    "37": "ERROR_NOT_A_FUNCTION",
    "38": "ERROR_INVALID_FORMAT",
    "39": "ERROR_TOO_MANY_ARGUMENTS",
    "40": "ERROR_WRONG_ARGUMENTS",
    "41": "ERROR_WRONG_RETURN_TYPE",
    "42": "ERROR_DUPLICATED_STRUCTURE_MEMBER",
    "43": "ERROR_EMPTY_STRING",
    "44": "ERROR_DIVISION_BY_ZERO",
    "45": "ERROR_REGULAR_EXPRESSION_TOO_LARGE",
    "46": "ERROR_TOO_MANY_RE_FIBERS",
    "47": "ERROR_COULD_NOT_READ_PROCESS_MEMORY",
    "48": "ERROR_INVALID_EXTERNAL_VARIABLE_TYPE",
    "49": "ERROR_REGULAR_EXPRESSION_TOO_COMPLEX",
}

type_list = [
    "RAR self-extracting archive",
    "Nullsoft Installer self-extracting archive",
    "7-zip Installer data",
    "Inno Setup",
    "MSI Installer",
    "Microsoft Cabinet",  # ToDo add die support here
]


class PCAP:
    """PCAP base object."""

    def __init__(self, file_path):
        self.file_path = file_path


class URL:
    """URL base object."""

    def __init__(self, url):
        """@param url: URL"""
        self.url = url


class File:
    """Basic file object class with all useful utilities."""

    # The yara rules should not change during one Cuckoo run and as such we're
    # caching 'em. This dictionary is filled during init_yara().
    yara_rules = {}
    yara_initialized = False
    # static fields which indicate whether the user has been
    # notified about missing dependencies already
    notified_yara = False
    notified_pydeep = False

    def __init__(self, file_path, guest_paths=None, file_name=None):
        """@param file_path: file path."""
        self.file_name = file_name
        self.file_path = file_path
        self.file_path_ansii = file_path if isinstance(file_path, str) else file_path.decode()
        self.guest_paths = guest_paths
        self.path_object = Path(self.file_path_ansii)

        # these will be populated when first accessed
        self._file_data = None
        self._crc32 = None
        self._md5 = None
        self._sha1 = None
        self._sha256 = None
        self._sha512 = None
        self._pefile = False
        self.file_type = None
        self.pe = None

    def get_name(self):
        """Get file name.
        @return: file name.
        """
        return self.file_name or Path(self.file_path).name

    def valid(self):
        return self.path_object.exists() and self.path_object.is_file() and self.path_object.stat().st_size

    def get_data(self):
        """Read file contents.
        @return: data.
        """
        return self.file_data

    def get_chunks(self, size=16):
        """Read file contents in chunks (generator)."""
        chunk_size = size * 1024
        with open(self.file_path, "rb") as fd:
            while True:
                chunk = fd.read(chunk_size)
                if not chunk:
                    break
                yield chunk

    def calc_hashes(self):
        """Calculate all possible hashes for this file."""
        crc = 0
        md5 = hashlib.md5()
        sha1 = hashlib.sha1()
        sha256 = hashlib.sha256()
        sha512 = hashlib.sha512()
        sha3_384 = hashlib.sha3_384()

        if HAVE_TLSH:
            tlsh_hash = tlsh.Tlsh()

        for chunk in self.get_chunks():
            crc = binascii.crc32(chunk, crc)
            md5.update(chunk)
            sha1.update(chunk)
            sha256.update(chunk)
            sha512.update(chunk)
            sha3_384.update(chunk)
            if HAVE_TLSH:
                tlsh_hash.update(chunk)

        self._crc32 = "".join(f"{(crc >> i) & 0xFF:02X}" for i in (24, 16, 8, 0))
        self._md5 = md5.hexdigest()
        self._sha1 = sha1.hexdigest()
        self._sha256 = sha256.hexdigest()
        self._sha512 = sha512.hexdigest()
        self._sha3_384 = sha3_384.hexdigest()
        if HAVE_TLSH:
            with contextlib.suppress(ValueError):
                tlsh_hash.final()
                self._tlsh_hash = tlsh_hash.hexdigest()

    @property
    def file_data(self):
        if not self._file_data:
            if self.path_object.exists():
                self._file_data = self.path_object.read_bytes()
        return self._file_data

    def get_size(self):
        """Get file size.
        @return: file size.
        """
        return self.path_object.stat().st_size if self.path_object.exists() else 0

    def get_crc32(self):
        """Get CRC32.
        @return: CRC32.
        """
        if not self._crc32:
            self.calc_hashes()
        return self._crc32

    def get_md5(self):
        """Get MD5.
        @return: MD5.
        """
        if not self._md5:
            self.calc_hashes()
        return self._md5

    def get_sha1(self):
        """Get SHA1.
        @return: SHA1.
        """
        if not self._sha1:
            self.calc_hashes()
        return self._sha1

    def get_sha256(self):
        """Get SHA256.
        @return: SHA256.
        """
        if not self._sha256:
            self.calc_hashes()
        return self._sha256

    def get_sha512(self):
        """
        Get SHA512.
        @return: SHA512.
        """
        if not self._sha512:
            self.calc_hashes()
        return self._sha512

    def get_sha3_384(self):
        """
        Get SHA3_384.
        @return: SHA3_384.
        """
        if not self._sha3_384:
            self.calc_hashes()
        return self._sha3_384

    def get_ssdeep(self):
        """Get SSDEEP.
        @return: SSDEEP.
        """
        if not HAVE_PYDEEP:
            if not File.notified_pydeep:
                File.notified_pydeep = True
                log.warning("Unable to import pydeep (install with `pip3 install pydeep`)")
            return None

        try:
            return pydeep.hash_file(self.file_path).decode()
        except Exception:
            return None

    def get_content_type(self):
        """Get MIME content file type (example: image/jpeg).
        @return: file content type.
        """
        file_type = None
        if self.path_object.exists():
            if HAVE_MAGIC:
                fn = False
                if hasattr(magic, "detect_from_filename"):
                    fn = magic.detect_from_filename
                if hasattr(magic, "from_file"):
                    fn = magic.from_file
                if fn:
                    try:
                        file_type = fn(self.file_path_ansii)
                    except magic.MagicException as e:
                        log.error("Magic error: %s", str(e))
                    except Exception as e:
                        log.error(e, exc_info=True)
                if not file_type and hasattr(magic, "open"):
                    try:
                        ms = magic.open(magic.MAGIC_MIME | magic.MAGIC_SYMLINK)
                        ms.load()
                        file_type = ms.file(self.file_path)
                        ms.close()
                    except Exception as e:
                        log.error(e, exc_info=True)

            if file_type is None:
                try:
                    p = subprocess.Popen(
                        ["file", "-b", "-L", "--mime-type", self.file_path], universal_newlines=True, stdout=subprocess.PIPE
                    )
                    file_type = p.stdout.read().strip()
                except Exception as e:
                    log.error(e, exc_info=True)

        return file_type

    def get_type(self):
        """Get MIME file type.
        @return: file type.
        """
        if self.file_type:
            return self.file_type
        if self.file_path:
            try:
                if IsPEImage(self.file_data):
                    self._pefile = True
                    if HAVE_PEFILE:
                        try:
                            self.pe = pefile.PE(data=self.file_data, fast_load=True)
                        except pefile.PEFormatError:
                            self.file_type = "PE image for MS Windows"
                            log.debug("Unable to instantiate pefile on image: %s", self.file_path)
                        if self.pe:
                            is_dll = self.pe.is_dll()
                            is_x64 = self.pe.FILE_HEADER.Machine == IMAGE_FILE_MACHINE_AMD64
                            gui_type = "console" if self.pe.OPTIONAL_HEADER.Subsystem == 3 else "GUI"
                            dotnet_string = ""
                            with contextlib.suppress(AttributeError, IndexError):
                                dotnet_string = (
                                    " Mono/.Net assembly"
                                    if self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[
                                        pefile.DIRECTORY_ENTRY["IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR"]
                                    ].VirtualAddress
                                    != 0
                                    else ""
                                )
                            # Emulate magic for now
                            if is_dll and is_x64:
                                self.file_type = f"PE32+ executable (DLL) (GUI) x86-64{dotnet_string}, for MS Windows"
                            elif is_dll:
                                self.file_type = f"PE32 executable (DLL) (GUI) Intel 80386{dotnet_string}, for MS Windows"
                            elif is_x64:
                                self.file_type = f"PE32+ executable ({gui_type}) x86-64{dotnet_string}, for MS Windows"
                            else:
                                self.file_type = f"PE32 executable ({gui_type}) Intel 80386{dotnet_string}, for MS Windows"
                    elif not File.notified_pefile:
                        File.notified_pefile = True
                        log.warning("Unable to import pefile (install with `pip3 install pefile`)")
            except Exception as e:
                log.error(e, exc_info=True)
        if not self.file_type:
            self.file_type = self.get_content_type()

        return self.file_type

    def _yara_encode_string(self, yara_string):
        # Beware, spaghetti code ahead.
        if not isinstance(yara_string, bytes):
            return yara_string
        try:
            new = yara_string.decode()
        except UnicodeDecodeError:
            # yara_string = binascii.hexlify(yara_string.lstrip("uU")).upper()
            yara_string = binascii.hexlify(yara_string).upper()
            yara_string = b" ".join(yara_string[i : i + 2] for i in range(0, len(yara_string), 2))
            new = f"{{ {yara_string.decode()} }}"

        return new

    @classmethod
    def init_yara(self):
        """Generates index for yara signatures."""

        categories = ("binaries", "urls", "memory", "CAPE", "macro", "monitor")
        log.debug("Initializing Yara...")

        # Generate root directory for yara rules.
        yara_root = os.path.join(CUCKOO_ROOT, "data", "yara")
        custom_yara_root = os.path.join(CUCKOO_ROOT, "custom", "yara")
        # Loop through all categories.
        for category in categories:
            rules, indexed = {}, []
            # Check if there is a directory for the given category.
            for path in (yara_root, custom_yara_root):
                category_root = os.path.join(path, category)
                if not path_exists(category_root):
                    log.warning("Missing Yara directory: %s?", category_root)
                    continue

                for filename in os.listdir(category_root):
                    if not filename.endswith((".yar", ".yara")):
                        continue
                    filepath = os.path.join(category_root, filename)
                    rules[f"rule_{category}_{len(rules)}"] = filepath
                    indexed.append(filename)

                # Need to define each external variable that will be used in the
            # future. Otherwise Yara will complain.
            externals = {"filename": ""}

            while True:
                try:
                    File.yara_rules[category] = yara.compile(filepaths=rules, externals=externals)
                    File.yara_initialized = True
                    break
                except yara.SyntaxError as e:
                    bad_rule = f"{str(e).split('.yar', 1)[0]}.yar"
                    log.debug(
                        "Trying to disable rule: %s. Can't compile it. Ensure that your YARA is properly installed.", bad_rule
                    )
                    if os.path.basename(bad_rule) not in indexed:
                        break
                    for k, v in rules.items():
                        if v == bad_rule:
                            del rules[k]
                            indexed.remove(os.path.basename(bad_rule))
                            log.error("Can't compile YARA rule: %s. Maybe is bad yara but can be missing YARA's module.", bad_rule)
                            break
                except yara.Error as e:
                    log.error("There was a syntax error in one or more Yara rules: %s", e)
                    break
            if category == "memory":
                try:
                    mem_rules = yara.compile(filepaths=rules, externals=externals)
                    mem_rules.save(os.path.join(yara_root, "index_memory.yarc"))
                except yara.Error as e:
                    if "could not open file" in str(e):
                        log.info("Can't write index_memory.yarc. Did you starting it with correct user?")
                    else:
                        log.error(e)

            indexed = sorted(indexed)
            for entry in indexed:
                if (category, entry) == indexed[-1]:
                    log.debug("\t `-- %s %s", category, entry)
                else:
                    log.debug("\t |-- %s %s", category, entry)

    def get_yara(self, category="binaries", externals=None):
        """Get Yara signatures matches.
        @return: matched Yara signatures.
        """
        if float(yara.__version__[:-2]) < 4.3:
            log.error("You using outdated YARA version. run: poetry install")
            return []

        if not File.yara_initialized:
            File.init_yara()

        results = []
        if not os.path.getsize(self.file_path):
            return results

        try:
            results, rule = [], File.yara_rules[category]
            for match in rule.match(self.file_path_ansii, externals=externals):
                strings = []
                addresses = {}
                for yara_string in match.strings:
                    for x in yara_string.instances:
                        strings.extend({self._yara_encode_string(x.matched_data)})
                        addresses.update({yara_string.identifier.strip("$"): x.offset})
                results.append(
                    {
                        "name": match.rule,
                        "meta": match.meta,
                        "strings": strings,
                        "addresses": addresses,
                    }
                )
        except Exception as e:
            errcode = str(e).rsplit(maxsplit=1)[-1]
            if errcode in yara_error:
                log.exception("Unable to match Yara signatures for %s: %s", self.file_path, yara_error[errcode])

            else:
                log.exception("Unable to match Yara signatures for %s: unknown code %s", self.file_path, errcode)

        return results

    cape_name_regex = re.compile(r" (?:payload|config|loader|strings)$", re.I)

    @classmethod
    def yara_hit_provides_detection(cls, hit: Dict[str, Any]) -> bool:
        cape_type = hit["meta"].get("cape_type", "")
        return bool(cls.cape_name_regex.search(cape_type))

    @classmethod
    def get_cape_name_from_yara_hit(cls, hit: Dict[str, Any]) -> str:
        """Use the cape_type as defined in the metadata for the yara hit
        (e.g. "SocGholish Payload") and return the part before
        "Loader", "Payload", "Config", or "Strings"
        """
        return cls.get_cape_name_from_cape_type(hit["meta"].get("cape_type", ""))

    @classmethod
    def get_cape_name_from_cape_type(cls, cape_type: str) -> str:
        """Return the part of the cape_type (e.g. "SocGholish Payload") preceding
        " Payload", " Config", " Loader", or " Strings"
        """
        return cls.cape_name_regex.sub("", cape_type)

    def get_tlsh(self):
        """
        Get TLSH.
        @return: TLSH.
        """
        if not hasattr(self, "_tlsh_hash"):
            return None
        if not self._tlsh_hash:
            self.calc_hashes()
        return self._tlsh_hash

    def get_rh_hash(self):
        if not self.pe:
            return None

        # source https://github.com/RichHeaderResearch/RichPE/blob/master/richpe.py#L34
        rich_header = self.pe.parse_rich_header()
        if rich_header is None:
            return None

        # Get list of @Comp.IDs and counts from Rich header
        # Elements in rich_fields at even indices are @Comp.IDs
        # Elements in rich_fields at odd indices are counts
        rich_fields = rich_header.get("values")
        if len(rich_fields) % 2 != 0:
            return None

        # The RichPE hash of a file is computed by computing the md5 of specific
        # metadata within  the Rich header and the PE header
        md5 = hashlib.md5()

        # Update hash using @Comp.IDs and masked counts from Rich header
        while len(rich_fields):
            compid = rich_fields.pop(0)
            count = rich_fields.pop(0)
            mask = 2 ** (count.bit_length() // 2 + 1) - 1
            count |= mask
            md5.update(struct.pack("<L", compid))
            md5.update(struct.pack("<L", count))

        # Update hash using metadata from the PE header
        md5.update(struct.pack("<L", self.pe.FILE_HEADER.Machine))
        md5.update(struct.pack("<L", self.pe.FILE_HEADER.Characteristics))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.Subsystem))
        md5.update(struct.pack("<B", self.pe.OPTIONAL_HEADER.MajorLinkerVersion))
        md5.update(struct.pack("<B", self.pe.OPTIONAL_HEADER.MinorLinkerVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MajorOperatingSystemVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MinorOperatingSystemVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MajorImageVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MinorImageVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MajorSubsystemVersion))
        md5.update(struct.pack("<L", self.pe.OPTIONAL_HEADER.MinorSubsystemVersion))

        # Close PE file and return RichPE hash digest
        return md5.hexdigest()

    def is_sfx(self):
        filetype = self.get_content_type()
        return any([ftype in filetype for ftype in type_list])

    def get_all_hashes(self):
        return {
            "crc32": self.get_crc32(),
            "md5": self.get_md5(),
            "sha1": self.get_sha1(),
            "sha256": self.get_sha256(),
            "sha512": self.get_sha512(),
            "rh_hash": self.get_rh_hash(),
            "ssdeep": self.get_ssdeep(),
            "tlsh": self.get_tlsh(),
            "sha3_384": self.get_sha3_384(),
        }

    def get_all(self):
        """Get all information available.
        @return: information dict.
        """

        infos = {
            "name": self.get_name(),
            "path": self.file_path,
            "guest_paths": self.guest_paths,
            "size": self.get_size(),
            "crc32": self.get_crc32(),
            "md5": self.get_md5(),
            "sha1": self.get_sha1(),
            "sha256": self.get_sha256(),
            "sha512": self.get_sha512(),
            "rh_hash": self.get_rh_hash(),
            "ssdeep": self.get_ssdeep(),
            "type": self.get_type(),
            "yara": self.get_yara(),
            "cape_yara": self.get_yara(category="CAPE"),
            "clamav": get_clamav(self.file_path),
            "tlsh": self.get_tlsh(),
            "sha3_384": self.get_sha3_384(),
        }

        return infos, self.pe


class Static(File):
    pass


class ProcDump:
    def __init__(self, dump_file, pretty=False):
        self._dumpfile = open(dump_file, "rb")
        self.dumpfile = mmap.mmap(self._dumpfile.fileno(), 0, access=mmap.ACCESS_READ)
        self.address_space = self.parse_dump()
        self.protmap = {
            PAGE_NOACCESS: "NOACCESS",
            PAGE_READONLY: "R",
            PAGE_READWRITE: "RW",
            PAGE_WRITECOPY: "RWC",
            PAGE_EXECUTE: "X",
            PAGE_EXECUTE_READ: "RX",
            PAGE_EXECUTE_READWRITE: "RWX",
            PAGE_EXECUTE_WRITECOPY: "RWXC",
        }

    def __del__(self):
        self.close()

    def close(self):
        if self.dumpfile:
            self.dumpfile.close()
            self.dumpfile = None
        if self._dumpfile:
            self._dumpfile.close()
            self._dumpfile = None

    def _prot_to_str(self, prot):
        if prot & PAGE_GUARD:
            return "G"
        prot &= 0xFF
        return self.protmap[prot]

    def pretty_print(self):
        new_addr_space = copy.deepcopy(self.address_space)
        for map in new_addr_space:
            map["start"] = f"0x{map['start']:08x}"
            map["end"] = f"0x{map['end']:08x}"
            map["size"] = f"0x{map['size']:08x}"
            if map["prot"] is None:
                map["prot"] = "Mixed"
            else:
                map["prot"] = self._prot_to_str(map["prot"])
            for chunk in map["chunks"]:
                chunk["start"] = f"0x{chunk['start']:08x}"
                chunk["end"] = f"0x{chunk['end']:08x}"
                chunk["size"] = f"0x{chunk['size']:08x}"
                chunk["prot"] = self._prot_to_str(chunk["prot"])
        return new_addr_space

    def _coalesce_chunks(self, chunklist):
        low = chunklist[0]["start"]
        high = chunklist[-1]["end"]
        prot = chunklist[0]["prot"]
        PE = chunklist[0]["PE"]
        for chunk in chunklist:
            if chunk["prot"] != prot:
                prot = None
        return {"start": low, "end": high, "size": high - low, "prot": prot, "PE": PE, "chunks": chunklist}

    def parse_dump(self):
        f = self.dumpfile
        address_space = []
        curchunk = []
        lastend = 0
        while True:
            data = f.read(24)
            if data == b"":
                break
            addr, size, mem_state, mem_type, mem_prot = struct.unpack("QIIII", data)
            offset = f.tell()
            if addr != lastend and len(curchunk):
                address_space.append(self._coalesce_chunks(curchunk))
                curchunk = []
            lastend = addr + size
            alloc = {
                "start": addr,
                "end": addr + size,
                "size": size,
                "prot": mem_prot,
                "state": mem_state,
                "type": mem_type,
                "offset": offset,
                "PE": False,
            }

            try:
                if f.read(2) == b"MZ":
                    alloc["PE"] = True
                f.seek(size - 2, 1)
            except Exception:
                break
            curchunk.append(alloc)
        if len(curchunk):
            address_space.append(self._coalesce_chunks(curchunk))

        f.seek(0)

        return address_space

    def get_data(self, addr, size):
        for map in self.address_space:
            if addr < map["start"] or addr >= map["end"]:
                continue
            for chunk in map["chunks"]:
                if addr < chunk["start"] or addr >= chunk["end"]:
                    continue
                maxsize = chunk["start"] + chunk["size"] - addr
                if size > maxsize:
                    size = maxsize
                self.dumpfile.seek(chunk["offset"] + addr - chunk["start"])
                return self.dumpfile.read(size)

    def search(self, regex, flags=0, all=False):
        if all:
            result = {"detail": []}
            matches = []
            for map in self.address_space:
                for chunk in map["chunks"]:
                    self.dumpfile.seek(chunk["offset"])
                    match = re.finditer(regex, self.dumpfile.read(chunk["end"] - chunk["start"]), flags)
                    thismatch = []
                    with contextlib.suppress(StopIteration):
                        while True:
                            m = next(match)
                            thismatch.append(m)
                            matches.append(m.group(0))
                    if thismatch:
                        result["detail"].append({"match": thismatch, "chunk": chunk})
            result["matches"] = matches
            return result
        else:
            for map in self.address_space:
                for chunk in map["chunks"]:
                    self.dumpfile.seek(chunk["offset"])
                    match = re.search(regex, self.dumpfile.read(chunk["end"] - chunk["start"]), flags)
                    if match:
                        return {"match": match, "chunk": chunk}
